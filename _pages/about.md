---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Hongyu Wang (王鸿钰 in Chinese), a third-year Ph.D candidate at Institute of Computing Technology, Chinese Academy of Sciences (ICT, CAS) under the supervision of Professor [Xilin Chen](http://vipl.ict.ac.cn/people/_xlchen/). I received my B.Eng. degree from Department of Computer Science and Technology, University of Science and Technology of China (USTC). I was advised by associate researcher [Chao Qian](http://www.lamda.nju.edu.cn/qianc/) at USTC. I am a research intern under the supervision of Dr. [Furu Wei](https://thegenerality.com/) and [Shuming Ma](https://scholar.google.com/citations?user=J44tjDMAAAAJ) at General Artificial Intelligence group (GenAI), MSR-Asia from Aug. 2021 to present.

*I have great interest on the following topics*: 
1. *Scale efficiently! Efficient architecture for the large-scale foundation models*
2. *Multimodal reasoning, robotics*

Contact: why0711@mail.ustc.edu.cn

## News:
- **[04/2025]** Introducing [<span style="color:red;"><strong>BitNet b1.58 2B4T</strong></span>](https://arxiv.org/abs/2411.04965), the first native 1-bit LLM trained at scale! [Model weights](https://huggingface.co/collections/microsoft/bitnet-67fddfe39a03686367734550) and [technical report](https://arxiv.org/abs/2504.12285) are public! Cooking larger models now...
- **[11/2024]** [<span style="color:red;"><strong>BitNet a4.8</strong></span>](https://arxiv.org/abs/2411.04965), enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 has only 55% active parameters and further supports 3-bit KV cache without extra training.
- **[10/2024]** [<span style="color:red;"><strong>bitnet.cpp</strong></span>](https://github.com/microsoft/bitnet), the official inference framework for BitNet b1.58! <b>Run a 100B BitNet b1.58 model on a single CPU at a human reading speed!</b>
- **[07/2024]** [<span style="color:red;"><strong>Q-Sparse</strong></span>](https://arxiv.org/abs/2407.10969), the fully Sparsely-Activated LLM.
- **[04/2024]** <b>DeepNet</b> is accepted as the regular paper by TPAMI 2024.
- **[03/2024]** [<span style="color:red;"><strong>BitNet b1.58: Training Tips, Code and FAQ. </strong></span>](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- **[02/2024]** [<span style="color:red;"><strong>BitNet b1.58</strong></span>](https://arxiv.org/pdf/2402.17764.pdf), the first ternary LLM that matches the performance of FP16 LLM with siginificant reduction of inference cost (latency, memory, throughput, and energy consumption)
- **[10/2023]** [**BitNet**](https://arxiv.org/pdf/2310.11453.pdf), the first binary LLM that has competitive performance of FP16 LLM and SoTA 8-bit quantization methods
- **[05/2023]** Magneto is accepted by ICML 2023
- **[11/2022]** [TorchScale](https://github.com/microsoft/torchscale): Transformers at Scale
- **[10/2022]** [**Magneto**](https://arxiv.org/pdf/2210.06423.pdf), foundation Transformer, outperforms the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
- **[03/2022]** [**DeepNet**](https://arxiv.org/pdf/2203.00555.pdf), Scaling Transformers to 1,000 Layers! Outperform M2M-100 by 5 BLEU point on the massive multilingual benchmarks.
- **[08/2021]** Start my internship at MSR-Asia ~

