---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Hongyu Wang (王鸿钰 in Chinese), a third-year Ph.D candidate at Chinese Academy of Sciences (CAS). I received my B.Eng. degree from Department of Computer Science and Technology, University of Science and Technology of China (USTC). I was advised by associate researcher [Chao Qian](http://www.lamda.nju.edu.cn/qianc/) at USTC. I was a research intern under the supervision of Dr. [Furu Wei](https://thegenerality.com/) and [Shuming Ma](https://shumingma.com/) at Natural Language Computing group, MSR-Asia from Aug. 2021 to Jan. 2023.

*I have great interest on the following topics*: 
1. *Scale efficiently! Efficient architecture for the large-scale foundation models*
2. *Expert-level multimodal reasoning and understanding*

Contact: why0711@mail.ustc.edu.cn

## News:
- **[07/2024]** [<span style="color:red;"><strong>Q-Sparse</strong></span>](https://arxiv.org/abs/2407.10969), the fully Sparsely-Activated LLM.
- **[04/2024]** <b>DeepNet</b> is accepted as the regular paper by TPAMI 2024.
- **[03/2024]** [<span style="color:red;"><strong>BitNet b1.58: Training Tips, Code and FAQ. </strong></span>](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- **[02/2024]** [<span style="color:red;"><strong>BitNet b1.58</strong></span>](https://arxiv.org/pdf/2402.17764.pdf), the first ternary LLM that matches the performance of FP16 LLM with siginificant reduction of inference cost (latency, memory, throughput, and energy consumption)
- **[10/2023]** [<span style="color:red;"><strong>BitNet</strong></span>](https://arxiv.org/pdf/2310.11453.pdf), the first binary LLM that has competitive performance of FP16 LLM and SoTA 8-bit quantization methods
- **[05/2023]** Magneto is accepted by ICML 2023
- **[11/2022]** [TorchScale](https://github.com/microsoft/torchscale): Transformers at Scale
- **[10/2022]** [**Magneto**](https://arxiv.org/pdf/2210.06423.pdf), foundation Transformer, outperforms the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
- **[03/2022]** [**DeepNet**](https://arxiv.org/pdf/2203.00555.pdf), Scaling Transformers to 1,000 Layers! Outperform M2M-100 by 5 BLEU point on the massive multilingual benchmarks.
- **[08/2021]** Start my internship at MSR-Asia ~

