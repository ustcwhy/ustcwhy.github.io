---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* Bachelor of Computer Science and Technology, University of Science and Technology of China (USTC), Sept. 2018 - June 2022
* Ph.D student at Institute of Computing Technology, Chinese Academy of Sciences, Sept. 2022 - present

Work experience
======
* Aug. 2021 - Jan. 2023 & Aug. 2023 - June 2025: Research intern
  * GenAI group, Microsoft Research Asia
  * Supervisor: Dr. [Furu Wei](https://thegenerality.com/) and [Shuming Ma](https://scholar.google.com/citations?user=J44tjDMAAAAJ)

* Feb. 2021 - July 2021: Teaching Assistant for Mathematical analysis B2, undergraduate course
  * USTC
  * Supervisor: Professor Yelong Zheng

* Sept. 2020 - Jan. 2021: Teaching Assistant for Mathematical analysis B1, undergraduate course
  * USTC
  * Supervisor: Professor Yelong Zheng

Awards
======
* Outstanding Teaching Assistant at USTC, 2021
* Silver Award of Outstanding Student Scholarship at USTC, 2020
* Huawei Scholarship, 2020
* Silver Award of Outstanding Student Scholarship at USTC, 2019

Services
======
* Journal Reviewer: IEEE Transactions on Affective Computing.
* Conference Reviewer: ICLR, ICCV, ECCV, EMNLP, ACL Rolling Review.

Talks
======
* (05/2024) The Era of 1-bit LLM (BitNet b1 / b1.58 / a4.8 / v2 ) at Xiaomi LLM-Core. [Slides](https://github.com/ustcwhy/ustcwhy.github.io/blob/master/files/bitnet-20250527.pdf).
* (08/2024) The Era of 1-bit LLM at [MSR Asia Intern Tech Fest](https://mp.weixin.qq.com/s/HVhOhWpq1092Z5byc5nISw).
* (06/2024) The Era of 1-bit LLM at [Cohere for AI](https://www.youtube.com/watch?v=oxQjGOUbQx4&list=PLLalUvky4CLJKDaiWCumhsJpHNDhZeVll&index=17&t=229s), and SIGMA Lab, Tsinghua University.
* (04/2024) BitNet b1.58 at MiraclePlus.
* (08/2023) Magneto at [Microsoft Research Asia](https://www.msra.cn/zh-cn/news/features/icml-2023).

Preprints & Publications
======
## Scalable and Efficient Foundation Model
* [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models.](https://ustcwhy.github.io/publications/mote) <b>Hongyu Wang</b>, Jiayu Xu, Ruiping Wang, Yan Feng, Yitao Zhai, Peng Pei, Xunliang Cai, Xilin Chen.
* [BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation.](https://ustcwhy.github.io/publications/bitvla) <b>Hongyu Wang</b>, Chuyan Xiong, Ruiping Wang, Xilin Chen.
* [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs.](https://ustcwhy.github.io/publications/bitnet_v2/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Furu Wei.
* [BitNet b1.58 2B4T Technical Report.](https://ustcwhy.github.io/publications/bitnet_2b4t/) Shuming Ma\*, <b>Hongyu Wang\*</b>, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, Furu Wei
* [BitNet a4.8: 4-bit Activations for 1-bit LLMs.](https://ustcwhy.github.io/publications/bitnet_a4_8/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Furu Wei.
* [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs.](https://ustcwhy.github.io/publications/bitnet_cpp/) Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, <b>Hongyu Wang</b>, Furu Wei. ACL 2025.
* [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated.](https://ustcwhy.github.io/publications/qsparse/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Ruiping Wang, Furu Wei.
* [DeepNet: Scaling Transformers to 1,000 Layers.](https://ustcwhy.github.io/publications/deepnet/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2024).
* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits.](https://ustcwhy.github.io/publications/bitnet_b1_58) Shuming Ma\*, <b>Hongyu Wang\*</b>, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei.
* [BitNet: Scaling 1-bit Transformers for Large Language Models.](https://ustcwhy.github.io/publications/bitnet) <b>Hongyu Wang\*</b>, Shuming Ma\*, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei.
* [Magneto: A Foundation Transformer.](https://ustcwhy.github.io/publications/foundation_transformer/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei. International Conference on Machine Learning (ICML), 2023.
* [TorchScale: Transformers at Scale.](https://ustcwhy.github.io/publications/torchscale/) Shuming Ma\*, <b>Hongyu Wang\*</b>, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, Furu Wei.

## Multimodal and Robotics
* [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models.](https://ustcwhy.github.io/publications/m4u/) <b>Hongyu Wang\*</b>, Jiayu Xu\*, Senwei Xie\*, Ruiping Wang, Jialin Li, Zhaojie Xie, Bin Zhang, Chuyan Xiong, Xilin Chen.
* [Robotic Programmer: Video Instructed Policy Code Generation for Robotic Manipulation.](https://ustcwhy.github.io/publications/robopro/) Senwei Xie\*, <b>Hongyu Wang\*</b>, Zhanqi Xiao\*, Ruiping Wang, Xilin Chen. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025.


Media Reports
======
* **BitNet v2**: [微软再放LLM量化大招！原生4bit量化，成本暴减，性能几乎0损失 (新智元)](https://mp.weixin.qq.com/s/CafL3szFrBMuISRG0GUpWQ); [微软1bit LLM新研究：原生4bit激活值量化，可充分利用新一代GPU对4bit计算的原生支持 (量子位)](https://mp.weixin.qq.com/s/HlSDd3Tl5lK4sHSm25z9XQ)
* **BitNet b1.58 2B4T**: [Microsoft researchers say they’ve developed a hyper-efficient AI model that can run on CPUs. (TechCrunch)](https://techcrunch.com/2025/04/16/microsoft-researchers-say-theyve-developed-a-hyper-efficient-ai-model-that-can-run-on-cpus/), [Microsoft Fiscal Year 2025 Third Quarter Earnings Conference Call](https://www.microsoft.com/en-us/investor/events/fy-2025/earnings-fy-2025-q3), [仅需0.4GB，参数只有0和±1！微软开源首个原生1 bit模型，CPU轻松跑 (新智元)](https://mp.weixin.qq.com/s/G9ZbMnBVbeH1m45HY2JIKA), [微软开源“原生1bit”三进制LLM：2B参数，0.4GB内存/单CPU就能跑，性能与同规模全精度开源模型相当 (量子位)](https://mp.weixin.qq.com/s/CpHcrSpzoDYcagknX9oe5g)
* **BitNet a4.8**: [How Microsoft’s next-gen BitNet architecture is turbocharging LLM efficiency (VentureBeat)](https://venturebeat.com/ai/how-microsofts-next-gen-bitnet-architecture-is-turbocharging-llm-efficiency/), [1-bit大模型还能再突破！新一代BitNet架构启用4位激活值 (新智元)](https://mp.weixin.qq.com/s/aw3iXwNVypyrq7jnAgGoug)
* **BitNet.cpp**: [微软开源爆火1.58bit大模型推理框架！千亿参数模型量化后单CPU可跑，速度每秒5-7个token (量子位)](https://mp.weixin.qq.com/s/gerCRxj4eULOut9PtMlNog)
* **Q-Sparse**: [完全激活稀疏大模型，Q-Sparse突破LLM推理效能 (MSRA)](https://mp.weixin.qq.com/s/JlvfBXLgn_aS9GrhhAYncQ), [只激活3.8B参数，性能比肩同款7B模型！训练微调都能用，来自微软 (新智元)](https://mp.weixin.qq.com/s/hBC9TcYrHMGVG9VgogLqWw)
* **BitNet b1.58**: [1-bit LLMs Could Solve AI’s Energy Demands (IEEE Spectrum)](https://spectrum.ieee.org/1-bit-llm), [Small Bits, Big Ideas: The Amazing Rise Of 1-Bit LLMs For Building Faster And Slimmer Generative AI Apps (Forbes)](https://www.forbes.com/sites/lanceeliot/2024/11/22/small-bits-big-ideas-the-amazing-rise-of-1-bit-llms-for-building-faster-and-slimer-generative-ai-apps/), [微软6页论文爆火：三进制LLM，真香！(量子位)](https://mp.weixin.qq.com/s/ziQDq8eaFCKlMaMKV9EM8Q), [微软、国科大开启1Bit时代：大模型转三进制，速度快4倍能耗降至1/41 (机器之心)](https://mp.weixin.qq.com/s/ao71aBUsEXoO_DC3hwpqQA), [BitNet b1.58：开启1-bit大语言模型时代 (MSRA)](https://mp.weixin.qq.com/s/4qtD_S_cC8OF0GENBPP-_Q), [xTech (Japanese)](https://xtech.nikkei.com/atcl/nxt/column/18/00001/10028/)
* **TorchScale**: [让天下没有难训练的大模型，微软亚洲研究院开源TorchScale (MSRA)](https://mp.weixin.qq.com/s/7oSv-RlwpWRPy5-t8meKCA)
* **DeepNet**: [Transformer深至1000层还能稳定训练，微软实习生一作，LSTM之父转发 (量子位)](https://mp.weixin.qq.com/s/3cN5I1hqPZNe6cXUPJ2wVA), [解决训练难题，1000层的Transformer来了，训练代码很快公开 (机器之心)](https://mp.weixin.qq.com/s/ejXE4-oBkqqtYITKZHpudQ), [千层Transformer问世！多语种机器翻译水准刷新多项SOTA (新智元)](https://mp.weixin.qq.com/s/Vo-mlDMjYQXmwAsXkhF3Yg)

  
<!-- Talks
======
  <ul>{% for post in site.talks %}
    {% include archive-single-talk-cv.html %}
  {% endfor %}</ul>
  
Teaching
======
  <ul>{% for post in site.teaching %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
  
Service and leadership
======
* Currently signed in to 43 different slack teams -->
