---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* Bachelor of Computer Science and Technology, University of Science and Technology of China (USTC), Sept. 2018 - June 2022
* Ph.D student at Chinese Academy of Sciences, Sept. 2022 - present

Work experience
======
* Aug. 2021 - Jan. 2023 & Aug. 2023 - Present: Research intern
  * NLC group, Microsoft Research Asia
  * Supervisor: Dr. [Furu Wei](https://thegenerality.com/) and [Shuming Ma](https://shumingma.com/)

* Feb. 2021 - July 2021: Teaching Assistant for Mathematical analysis B2, undergraduate course
  * USTC
  * Supervisor: Professor Yelong Zheng

* Sept. 2020 - Jan. 2021: Teaching Assistant for Mathematical analysis B1, undergraduate course
  * USTC
  * Supervisor: Professor Yelong Zheng

Awards
======
* Outstanding Teaching Assistant at USTC, 2021
* Silver Award of Outstanding Student Scholarship at USTC, 2020
* Huawei Scholarship, 2020
* Silver Award of Outstanding Student Scholarship at USTC, 2019

Services
======
* Journal Reviewer: IEEE Transactions on Affective Computing.
* Conference Reviewer: ICLR, ECCV, EMNLP, ACL Rolling Review.

Talks
======
* (08/2024) The Era of 1-bit LLM at [MSR Asia Intern Tech Fest](https://mp.weixin.qq.com/s/HVhOhWpq1092Z5byc5nISw).
* (06/2024) The Era of 1-bit LLM at [Cohere for AI](https://www.youtube.com/watch?v=oxQjGOUbQx4&list=PLLalUvky4CLJKDaiWCumhsJpHNDhZeVll&index=17&t=229s), and SIGMA Lab, Tsinghua University.
* (04/2024) BitNet b1.58 at MiraclePlus.
* (08/2023) Magneto at [Microsoft Research Asia](https://www.msra.cn/zh-cn/news/features/icml-2023).

Preprints & Publications
======
* [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated.](https://ustcwhy.github.io/publications/qsparse/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Ruiping Wang, Furu Wei
* [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models.](https://ustcwhy.github.io/publications/m4u/) <b>Hongyu Wang\*</b>, Jiayu Xu\*, Senwei Xie\*, Ruiping Wang, Jialin Li, Zhaojie Xie, Bin Zhang, Chuyan Xiong, Xilin Chen
* [DeepNet: Scaling Transformers to 1,000 Layers.](https://ustcwhy.github.io/publications/deepnet/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2024)
* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits.](https://ustcwhy.github.io/publications/bitnet_b1_58) Shuming Ma\*, <b>Hongyu Wang\*</b>, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei
* [BitNet: Scaling 1-bit Transformers for Large Language Models.](https://ustcwhy.github.io/publications/bitnet) <b>Hongyu Wang\*</b>, Shuming Ma\*, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei
* [Magneto: A Foundation Transformer.](https://ustcwhy.github.io/publications/foundation_transformer/) <b>Hongyu Wang\*</b>, Shuming Ma\*, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei. International Conference on Machine Learning (ICML), 2023.
* [TorchScale: Transformers at Scale.](https://ustcwhy.github.io/publications/torchscale/) Shuming Ma\*, <b>Hongyu Wang\*</b>, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, Furu Wei
  
<!-- Talks
======
  <ul>{% for post in site.talks %}
    {% include archive-single-talk-cv.html %}
  {% endfor %}</ul>
  
Teaching
======
  <ul>{% for post in site.teaching %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul>
  
Service and leadership
======
* Currently signed in to 43 different slack teams -->
